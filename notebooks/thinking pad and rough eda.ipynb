{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a cleaning / preporcessing pipeline as a function\n",
    "- split the data\n",
    "- do eda\n",
    "- find profanity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "#import advertools as adv\n",
    "import demoji\n",
    "import json\n",
    "import pandas as pd\n",
    "import re, nltk\n",
    "import warnings\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_word = stopwords.words('english')\n",
    "warnings.filterwarnings(action=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/cyberbullying_tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying',\n",
       "       'age', 'ethnicity'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cyberbullying_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47692, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data cleaning steps\n",
    "\n",
    "- do eda on all the datzset \n",
    "- replace emojis with word\n",
    "- keep hastags, but remove # \n",
    "- remove usernames \n",
    "- remove punctuation\n",
    "- remove stop words and two letter words\n",
    "- tokenize\n",
    "\n",
    "\n",
    "- find profanity words using word2vec or not <>\n",
    "- add more words to the list of profanity???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_clean(df):\n",
    "\n",
    "    # Opening JSON file\n",
    "    f = open('../data/external/profanity_list.json') \n",
    "    profanity_data = json.load(f)\n",
    "\n",
    "    # extract emojis\n",
    "    def extract_emoji(txt):\n",
    "        emoji_txt = demoji.findall(txt)\n",
    "        emoji_keys = emoji_txt.keys()\n",
    "        emoji_values = emoji_txt.values()\n",
    "        return  ' '.join(list(map(str, emoji_keys))), ' '.join(list(map(str, emoji_values)))\n",
    "\n",
    "    # extract hashtags\n",
    "    def hashtags(txt):\n",
    "        txt = re.findall(\"#([a-zA-Z0-9_]{1,50})\", txt)\n",
    "        return ' '.join(list(map(str, txt)))\n",
    "    \n",
    "    # extract mentions\n",
    "    def mentions(txt):\n",
    "        txt = re.findall(\"@([a-zA-Z0-9_]{1,50})\", txt)\n",
    "        return ' '.join(list(map(str, txt)))\n",
    "\n",
    "    # lookup profanity manually\n",
    "    def find_profanity(text):\n",
    "        profanity_set = set() \n",
    "        for word in text.split(): \n",
    "            if word in profanity_data:\n",
    "                profanity_set.add(word)\n",
    "        return ' '.join(list(map(str, profanity_set)))\n",
    "\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "        \n",
    "    df['profanity_list'] = df['tweet_text'].apply(lambda x: find_profanity(x))\n",
    "    df['hashtags'] = df['tweet_text'].apply(lambda x: hashtags(x))\n",
    "    df['mentions'] = df['tweet_text'].apply(lambda x: mentions(x))\n",
    "    df['emoji_face'] =  df['tweet_text'].apply(lambda x: extract_emoji(x)[0])\n",
    "    df['emoji_names'] =  df['tweet_text'].apply(lambda x:extract_emoji(x)[1])\n",
    "\n",
    "    # remove stopwords\n",
    "    df['clean_txt'] = df['tweet_text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_word or len(word) > 2))\n",
    "    df['clean_txt'] =  df['clean_txt'].apply(lambda x: re.sub(\"@[A-Za-z0-9_]+\",\"\", x))\n",
    "    #remove hashtags\n",
    "    df['clean_txt'] =  df['clean_txt'].apply(lambda x: re.sub(\"#[A-Za-z0-9_]+\",\"\", x))\n",
    "\n",
    "    df['clean_txt_emoji'] =   df['clean_txt'] + \" \" + df['emoji_names'] # add emoji as text to tweet\n",
    "    df['clean_txt_emoji'] =  df['clean_txt_emoji'].str.lower() # to lowe case\n",
    "    df['clean_txt_emoji'] = df['clean_txt_emoji'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_word)) #remove stop words\n",
    "    df['clean_txt_emoji'] = df['clean_txt_emoji'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x)) # remove links\n",
    "    df['clean_txt_emoji'] = df['clean_txt_emoji'].str.replace(\"[^a-zA-Z#]\", \" \") # puntuation\n",
    "    df['clean_txt_emoji'] = df['clean_txt_emoji'].apply(lambda x: ' '.join(word for word in x.split() if len(word) > 2))\n",
    "    df['tokenized'] = df['clean_txt_emoji'].apply(word_tokenize) # tokenize\n",
    "    df['pos_tags'] = df['tokenized'].apply(nltk.tag.pos_tag) # add pos tag\n",
    "    df['wordnet_pos'] = df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x]) # extract a new pos tag\n",
    "    df['lemmatized'] = df['wordnet_pos'].apply(lambda x: [ lemmatizer.lemmatize(word, tag) for word, tag in x]) # lemmatize\n",
    "   \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning_time < 5 minutes\n",
    "clean_df = super_clean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After cleaning \n",
    "- Lemmatizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>profanity_list</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>emoji_face</th>\n",
       "      <th>emoji_names</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>clean_txt_emoji</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8885</th>\n",
       "      <td>RT @1scrag1: I talk with a number of single Am...</td>\n",
       "      <td>gender</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1scrag1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>RT : I talk with number single American ladies...</td>\n",
       "      <td>talk number single american ladies can underst...</td>\n",
       "      <td>[talk, number, single, american, ladies, can, ...</td>\n",
       "      <td>[(talk, NN), (number, NN), (single, JJ), (amer...</td>\n",
       "      <td>[(talk, n), (number, n), (single, a), (america...</td>\n",
       "      <td>[talk, number, single, american, lady, can, un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27229</th>\n",
       "      <td>@jonrosenberg just checking. :)</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>jonrosenberg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>just checking. :)</td>\n",
       "      <td>checking</td>\n",
       "      <td>[checking]</td>\n",
       "      <td>[(checking, VBG)]</td>\n",
       "      <td>[(checking, v)]</td>\n",
       "      <td>[check]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42190</th>\n",
       "      <td>Excuse my language, but I‚Äôve heard of this thi...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>blacks</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Excuse language, but I‚Äôve heard this thing cal...</td>\n",
       "      <td>excuse language heard thing called magical neg...</td>\n",
       "      <td>[excuse, language, heard, thing, called, magic...</td>\n",
       "      <td>[(excuse, NN), (language, NN), (heard, VBD), (...</td>\n",
       "      <td>[(excuse, n), (language, n), (heard, v), (thin...</td>\n",
       "      <td>[excuse, language, hear, thing, call, magical,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22593</th>\n",
       "      <td>Please check how many Muslims carried out terr...</td>\n",
       "      <td>religion</td>\n",
       "      <td>terrorist</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Please check how many Muslims carried out terr...</td>\n",
       "      <td>please check many muslims carried terrorist at...</td>\n",
       "      <td>[please, check, many, muslims, carried, terror...</td>\n",
       "      <td>[(please, VB), (check, VB), (many, JJ), (musli...</td>\n",
       "      <td>[(please, v), (check, v), (many, a), (muslims,...</td>\n",
       "      <td>[please, check, many, muslim, carry, terrorist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text   cyberbullying_type  \\\n",
       "8885   RT @1scrag1: I talk with a number of single Am...               gender   \n",
       "27229                    @jonrosenberg just checking. :)  other_cyberbullying   \n",
       "42190  Excuse my language, but I‚Äôve heard of this thi...            ethnicity   \n",
       "22593  Please check how many Muslims carried out terr...             religion   \n",
       "\n",
       "      profanity_list hashtags      mentions emoji_face emoji_names  \\\n",
       "8885                                1scrag1                          \n",
       "27229                          jonrosenberg                          \n",
       "42190         blacks                                                 \n",
       "22593      terrorist                                                 \n",
       "\n",
       "                                               clean_txt  \\\n",
       "8885   RT : I talk with number single American ladies...   \n",
       "27229                                  just checking. :)   \n",
       "42190  Excuse language, but I‚Äôve heard this thing cal...   \n",
       "22593  Please check how many Muslims carried out terr...   \n",
       "\n",
       "                                         clean_txt_emoji  \\\n",
       "8885   talk number single american ladies can underst...   \n",
       "27229                                           checking   \n",
       "42190  excuse language heard thing called magical neg...   \n",
       "22593  please check many muslims carried terrorist at...   \n",
       "\n",
       "                                               tokenized  \\\n",
       "8885   [talk, number, single, american, ladies, can, ...   \n",
       "27229                                         [checking]   \n",
       "42190  [excuse, language, heard, thing, called, magic...   \n",
       "22593  [please, check, many, muslims, carried, terror...   \n",
       "\n",
       "                                                pos_tags  \\\n",
       "8885   [(talk, NN), (number, NN), (single, JJ), (amer...   \n",
       "27229                                  [(checking, VBG)]   \n",
       "42190  [(excuse, NN), (language, NN), (heard, VBD), (...   \n",
       "22593  [(please, VB), (check, VB), (many, JJ), (musli...   \n",
       "\n",
       "                                             wordnet_pos  \\\n",
       "8885   [(talk, n), (number, n), (single, a), (america...   \n",
       "27229                                    [(checking, v)]   \n",
       "42190  [(excuse, n), (language, n), (heard, v), (thin...   \n",
       "22593  [(please, v), (check, v), (many, a), (muslims,...   \n",
       "\n",
       "                                              lemmatized  \n",
       "8885   [talk, number, single, american, lady, can, un...  \n",
       "27229                                            [check]  \n",
       "42190  [excuse, language, hear, thing, call, magical,...  \n",
       "22593  [please, check, many, muslim, carry, terrorist...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text            I have to poop but I'm the only one at my regi...\n",
       "cyberbullying_type                                    not_cyberbullying\n",
       "profanity_list                                                         \n",
       "hashtags                                                            fml\n",
       "mentions                                                               \n",
       "emoji_face                                                             \n",
       "emoji_names                                                            \n",
       "clean_txt                    I have poop but I'm the only one register \n",
       "clean_txt_emoji                                       poop one register\n",
       "tokenized                                         [poop, one, register]\n",
       "pos_tags                        [(poop, NN), (one, CD), (register, NN)]\n",
       "wordnet_pos                        [(poop, n), (one, n), (register, n)]\n",
       "lemmatized                                        [poop, one, register]\n",
       "Name: 2640, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.iloc[2640]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>profanity_list</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>emoji_face</th>\n",
       "      <th>emoji_names</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>clean_txt_emoji</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Kids Loveüòò‚ù§ @ Mohamad Bin Zayed City ŸÖÿØŸäŸÜÿ© ŸÖÿ≠ŸÖ...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>‚ù§ üòò</td>\n",
       "      <td>red heart face blowing a kiss</td>\n",
       "      <td>Kids Loveüòò‚ù§ @ Mohamad Bin Zayed City ŸÖÿØŸäŸÜÿ© ŸÖÿ≠ŸÖ...</td>\n",
       "      <td>kids love mohamad bin zayed city red heart fac...</td>\n",
       "      <td>[kids, love, mohamad, bin, zayed, city, red, h...</td>\n",
       "      <td>[(kids, NNS), (love, VBP), (mohamad, JJ), (bin...</td>\n",
       "      <td>[(kids, n), (love, v), (mohamad, a), (bin, n),...</td>\n",
       "      <td>[kid, love, mohamad, bin, zayed, city, red, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>@iMrBarfield it starts Thursday. My classes ar...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>iMrBarfield</td>\n",
       "      <td>üòå</td>\n",
       "      <td>relieved face</td>\n",
       "      <td>starts Thursday. My classes are canceled for ...</td>\n",
       "      <td>starts thursday classes canceled tomorrow tho ...</td>\n",
       "      <td>[starts, thursday, classes, canceled, tomorrow...</td>\n",
       "      <td>[(starts, NNS), (thursday, NN), (classes, VBZ)...</td>\n",
       "      <td>[(starts, n), (thursday, n), (classes, v), (ca...</td>\n",
       "      <td>[start, thursday, class, cancel, tomorrow, tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>mmmm #MKR Forget Deconstruction @lisamromano @...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td>MKR</td>\n",
       "      <td>lisamromano garydlum mattsparks88</td>\n",
       "      <td>üçã</td>\n",
       "      <td>lemon</td>\n",
       "      <td>mmmm  Forget Deconstruction    THIS Lemon Tart...</td>\n",
       "      <td>mmmm forget deconstruction lemon tart mmmm lemon</td>\n",
       "      <td>[mmmm, forget, deconstruction, lemon, tart, mm...</td>\n",
       "      <td>[(mmmm, NN), (forget, VB), (deconstruction, NN...</td>\n",
       "      <td>[(mmmm, n), (forget, v), (deconstruction, n), ...</td>\n",
       "      <td>[mmmm, forget, deconstruction, lemon, tart, mm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>RT @mykitchenrules: Nawwww üò≠üò≠üò≠ #MKR</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td>MKR</td>\n",
       "      <td>mykitchenrules</td>\n",
       "      <td>üò≠</td>\n",
       "      <td>loudly crying face</td>\n",
       "      <td>RT : Nawwww üò≠üò≠üò≠</td>\n",
       "      <td>nawwww loudly crying face</td>\n",
       "      <td>[nawwww, loudly, crying, face]</td>\n",
       "      <td>[(nawwww, RB), (loudly, RB), (crying, VBG), (f...</td>\n",
       "      <td>[(nawwww, r), (loudly, r), (crying, v), (face,...</td>\n",
       "      <td>[nawwww, loudly, cry, face]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>@justinbieber Your How I Get Through Bullying ...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>justinbieber</td>\n",
       "      <td>‚ô•</td>\n",
       "      <td>heart suit</td>\n",
       "      <td>Your How I Get Through Bullying And Tough Tim...</td>\n",
       "      <td>get bullying tough times thinking got makes se...</td>\n",
       "      <td>[get, bullying, tough, times, thinking, got, m...</td>\n",
       "      <td>[(get, VB), (bullying, JJ), (tough, JJ), (time...</td>\n",
       "      <td>[(get, v), (bullying, a), (tough, a), (times, ...</td>\n",
       "      <td>[get, bullying, tough, time, think, get, make,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42964</th>\n",
       "      <td>RT @syeoga: Hella is from the bay....and major...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>bitches</td>\n",
       "      <td></td>\n",
       "      <td>syeoga</td>\n",
       "      <td>¬©</td>\n",
       "      <td>copyright</td>\n",
       "      <td>RT : Hella from the bay....and majority LA bit...</td>\n",
       "      <td>hella bay and majority bitches hate word hella...</td>\n",
       "      <td>[hella, bay, and, majority, bitches, hate, wor...</td>\n",
       "      <td>[(hella, NN), (bay, NN), (and, CC), (majority,...</td>\n",
       "      <td>[(hella, n), (bay, n), (and, n), (majority, n)...</td>\n",
       "      <td>[hella, bay, and, majority, bitch, hate, word,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46427</th>\n",
       "      <td>‚Äî @oibanai ije, ur one of my faves but u alr k...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>oibanai</td>\n",
       "      <td>‚ò∫</td>\n",
       "      <td>smiling face</td>\n",
       "      <td>‚Äî  ije, ur one faves but u alr know that hehe ...</td>\n",
       "      <td>ije one faves alr know hehe also love seeing t...</td>\n",
       "      <td>[ije, one, faves, alr, know, hehe, also, love,...</td>\n",
       "      <td>[(ije, VB), (one, CD), (faves, VBZ), (alr, RB)...</td>\n",
       "      <td>[(ije, v), (one, n), (faves, v), (alr, r), (kn...</td>\n",
       "      <td>[ije, one, faves, alr, know, hehe, also, love,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46879</th>\n",
       "      <td>@ksorbs üòÇüòÜüòÇ #classic #coon... As pawpaw would ...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td></td>\n",
       "      <td>classic coon momentsimissthatoldmanofhonor hil...</td>\n",
       "      <td>ksorbs</td>\n",
       "      <td>üòÜ üôä üòÇ</td>\n",
       "      <td>grinning squinting face speak-no-evil monkey f...</td>\n",
       "      <td>üòÇüòÜüòÇ  ... As pawpaw would DO üôä YEP I went ther...</td>\n",
       "      <td>pawpaw would yep went there grinning squinting...</td>\n",
       "      <td>[pawpaw, would, yep, went, there, grinning, sq...</td>\n",
       "      <td>[(pawpaw, NN), (would, MD), (yep, VB), (went, ...</td>\n",
       "      <td>[(pawpaw, n), (would, n), (yep, v), (went, v),...</td>\n",
       "      <td>[pawpaw, would, yep, go, there, grin, squint, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47610</th>\n",
       "      <td>@Lilcruz2 you really tryna get me in trouble o...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td></td>\n",
       "      <td>coon</td>\n",
       "      <td>Lilcruz2</td>\n",
       "      <td>üòÇ</td>\n",
       "      <td>face with tears of joy</td>\n",
       "      <td>you really tryna get trouble twitterüòÇüòÇüòÇ</td>\n",
       "      <td>really tryna get trouble twitter face tears joy</td>\n",
       "      <td>[really, tryna, get, trouble, twitter, face, t...</td>\n",
       "      <td>[(really, RB), (tryna, JJ), (get, NN), (troubl...</td>\n",
       "      <td>[(really, r), (tryna, a), (get, n), (trouble, ...</td>\n",
       "      <td>[really, tryna, get, trouble, twitter, face, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47675</th>\n",
       "      <td>Y'all make sure y'all following a nigga #coon‚õΩ...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>nigga</td>\n",
       "      <td>coon joeworld</td>\n",
       "      <td></td>\n",
       "      <td>üÜñ ‚õΩ üòπ üÖ∞ üëå üíØ</td>\n",
       "      <td>NG button fuel pump cat with tears of joy A bu...</td>\n",
       "      <td>Y'all make sure y'all following nigga ‚õΩüÖ∞üÜñüíØlilb...</td>\n",
       "      <td>all make sure all following nigga lilbitch but...</td>\n",
       "      <td>[all, make, sure, all, following, nigga, lilbi...</td>\n",
       "      <td>[(all, DT), (make, VBP), (sure, JJ), (all, DT)...</td>\n",
       "      <td>[(all, n), (make, v), (sure, a), (all, n), (fo...</td>\n",
       "      <td>[all, make, sure, all, following, nigga, lilbi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440 rows √ó 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text cyberbullying_type  \\\n",
       "21     Kids Loveüòò‚ù§ @ Mohamad Bin Zayed City ŸÖÿØŸäŸÜÿ© ŸÖÿ≠ŸÖ...  not_cyberbullying   \n",
       "80     @iMrBarfield it starts Thursday. My classes ar...  not_cyberbullying   \n",
       "135    mmmm #MKR Forget Deconstruction @lisamromano @...  not_cyberbullying   \n",
       "144                  RT @mykitchenrules: Nawwww üò≠üò≠üò≠ #MKR  not_cyberbullying   \n",
       "151    @justinbieber Your How I Get Through Bullying ...  not_cyberbullying   \n",
       "...                                                  ...                ...   \n",
       "42964  RT @syeoga: Hella is from the bay....and major...          ethnicity   \n",
       "46427  ‚Äî @oibanai ije, ur one of my faves but u alr k...          ethnicity   \n",
       "46879  @ksorbs üòÇüòÜüòÇ #classic #coon... As pawpaw would ...          ethnicity   \n",
       "47610  @Lilcruz2 you really tryna get me in trouble o...          ethnicity   \n",
       "47675  Y'all make sure y'all following a nigga #coon‚õΩ...          ethnicity   \n",
       "\n",
       "      profanity_list                                           hashtags  \\\n",
       "21                                                                        \n",
       "80                                                                        \n",
       "135                                                                 MKR   \n",
       "144                                                                 MKR   \n",
       "151                                                                       \n",
       "...              ...                                                ...   \n",
       "42964        bitches                                                      \n",
       "46427                                                                     \n",
       "46879                 classic coon momentsimissthatoldmanofhonor hil...   \n",
       "47610                                                              coon   \n",
       "47675          nigga                                      coon joeworld   \n",
       "\n",
       "                                mentions   emoji_face  \\\n",
       "21                                                ‚ù§ üòò   \n",
       "80                           iMrBarfield            üòå   \n",
       "135    lisamromano garydlum mattsparks88            üçã   \n",
       "144                       mykitchenrules            üò≠   \n",
       "151                         justinbieber            ‚ô•   \n",
       "...                                  ...          ...   \n",
       "42964                             syeoga            ¬©   \n",
       "46427                            oibanai            ‚ò∫   \n",
       "46879                             ksorbs        üòÜ üôä üòÇ   \n",
       "47610                           Lilcruz2            üòÇ   \n",
       "47675                                     üÜñ ‚õΩ üòπ üÖ∞ üëå üíØ   \n",
       "\n",
       "                                             emoji_names  \\\n",
       "21                         red heart face blowing a kiss   \n",
       "80                                         relieved face   \n",
       "135                                                lemon   \n",
       "144                                   loudly crying face   \n",
       "151                                           heart suit   \n",
       "...                                                  ...   \n",
       "42964                                          copyright   \n",
       "46427                                       smiling face   \n",
       "46879  grinning squinting face speak-no-evil monkey f...   \n",
       "47610                             face with tears of joy   \n",
       "47675  NG button fuel pump cat with tears of joy A bu...   \n",
       "\n",
       "                                               clean_txt  \\\n",
       "21     Kids Loveüòò‚ù§ @ Mohamad Bin Zayed City ŸÖÿØŸäŸÜÿ© ŸÖÿ≠ŸÖ...   \n",
       "80      starts Thursday. My classes are canceled for ...   \n",
       "135    mmmm  Forget Deconstruction    THIS Lemon Tart...   \n",
       "144                                     RT : Nawwww üò≠üò≠üò≠    \n",
       "151     Your How I Get Through Bullying And Tough Tim...   \n",
       "...                                                  ...   \n",
       "42964  RT : Hella from the bay....and majority LA bit...   \n",
       "46427  ‚Äî  ije, ur one faves but u alr know that hehe ...   \n",
       "46879   üòÇüòÜüòÇ  ... As pawpaw would DO üôä YEP I went ther...   \n",
       "47610           you really tryna get trouble twitterüòÇüòÇüòÇ    \n",
       "47675  Y'all make sure y'all following nigga ‚õΩüÖ∞üÜñüíØlilb...   \n",
       "\n",
       "                                         clean_txt_emoji  \\\n",
       "21     kids love mohamad bin zayed city red heart fac...   \n",
       "80     starts thursday classes canceled tomorrow tho ...   \n",
       "135     mmmm forget deconstruction lemon tart mmmm lemon   \n",
       "144                            nawwww loudly crying face   \n",
       "151    get bullying tough times thinking got makes se...   \n",
       "...                                                  ...   \n",
       "42964  hella bay and majority bitches hate word hella...   \n",
       "46427  ije one faves alr know hehe also love seeing t...   \n",
       "46879  pawpaw would yep went there grinning squinting...   \n",
       "47610    really tryna get trouble twitter face tears joy   \n",
       "47675  all make sure all following nigga lilbitch but...   \n",
       "\n",
       "                                               tokenized  \\\n",
       "21     [kids, love, mohamad, bin, zayed, city, red, h...   \n",
       "80     [starts, thursday, classes, canceled, tomorrow...   \n",
       "135    [mmmm, forget, deconstruction, lemon, tart, mm...   \n",
       "144                       [nawwww, loudly, crying, face]   \n",
       "151    [get, bullying, tough, times, thinking, got, m...   \n",
       "...                                                  ...   \n",
       "42964  [hella, bay, and, majority, bitches, hate, wor...   \n",
       "46427  [ije, one, faves, alr, know, hehe, also, love,...   \n",
       "46879  [pawpaw, would, yep, went, there, grinning, sq...   \n",
       "47610  [really, tryna, get, trouble, twitter, face, t...   \n",
       "47675  [all, make, sure, all, following, nigga, lilbi...   \n",
       "\n",
       "                                                pos_tags  \\\n",
       "21     [(kids, NNS), (love, VBP), (mohamad, JJ), (bin...   \n",
       "80     [(starts, NNS), (thursday, NN), (classes, VBZ)...   \n",
       "135    [(mmmm, NN), (forget, VB), (deconstruction, NN...   \n",
       "144    [(nawwww, RB), (loudly, RB), (crying, VBG), (f...   \n",
       "151    [(get, VB), (bullying, JJ), (tough, JJ), (time...   \n",
       "...                                                  ...   \n",
       "42964  [(hella, NN), (bay, NN), (and, CC), (majority,...   \n",
       "46427  [(ije, VB), (one, CD), (faves, VBZ), (alr, RB)...   \n",
       "46879  [(pawpaw, NN), (would, MD), (yep, VB), (went, ...   \n",
       "47610  [(really, RB), (tryna, JJ), (get, NN), (troubl...   \n",
       "47675  [(all, DT), (make, VBP), (sure, JJ), (all, DT)...   \n",
       "\n",
       "                                             wordnet_pos  \\\n",
       "21     [(kids, n), (love, v), (mohamad, a), (bin, n),...   \n",
       "80     [(starts, n), (thursday, n), (classes, v), (ca...   \n",
       "135    [(mmmm, n), (forget, v), (deconstruction, n), ...   \n",
       "144    [(nawwww, r), (loudly, r), (crying, v), (face,...   \n",
       "151    [(get, v), (bullying, a), (tough, a), (times, ...   \n",
       "...                                                  ...   \n",
       "42964  [(hella, n), (bay, n), (and, n), (majority, n)...   \n",
       "46427  [(ije, v), (one, n), (faves, v), (alr, r), (kn...   \n",
       "46879  [(pawpaw, n), (would, n), (yep, v), (went, v),...   \n",
       "47610  [(really, r), (tryna, a), (get, n), (trouble, ...   \n",
       "47675  [(all, n), (make, v), (sure, a), (all, n), (fo...   \n",
       "\n",
       "                                              lemmatized  \n",
       "21     [kid, love, mohamad, bin, zayed, city, red, he...  \n",
       "80     [start, thursday, class, cancel, tomorrow, tho...  \n",
       "135    [mmmm, forget, deconstruction, lemon, tart, mm...  \n",
       "144                          [nawwww, loudly, cry, face]  \n",
       "151    [get, bullying, tough, time, think, get, make,...  \n",
       "...                                                  ...  \n",
       "42964  [hella, bay, and, majority, bitch, hate, word,...  \n",
       "46427  [ije, one, faves, alr, know, hehe, also, love,...  \n",
       "46879  [pawpaw, would, yep, go, there, grin, squint, ...  \n",
       "47610  [really, tryna, get, trouble, twitter, face, t...  \n",
       "47675  [all, make, sure, all, following, nigga, lilbi...  \n",
       "\n",
       "[440 rows x 13 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[clean_df[\"emoji_face\"].str.len() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source for lemmatixation and tokenization : https://www.holisticseo.digital/python-seo/nltk/lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list =  [lemmatizer.lemmatize(token, \"v\") for token in word_tokenize(words)]\n",
    "    sentence = \" \".join(lemma_list)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(\"../data/processed/clean_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freak work please I have many things to do doesnt\n"
     ]
    }
   ],
   "source": [
    "ss = \"freaking work please I have many things to does doesnt\"\n",
    "#clean_df['clean_txt_lemma'] = clean_df['super_clean_txt'].apply(lambda x: lemma(x))\n",
    "print(lemma(ss))\n",
    "#clean_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whole me being republican thing must reall'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"whole me being republican thing must reall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text            Dr..ur genuine and cleaver journelist , first ...\n",
       "cyberbullying_type                                            ethnicity\n",
       "profanity_list                                                         \n",
       "hashtags                                                               \n",
       "mentions                                                               \n",
       "emoji_face                                                             \n",
       "emoji_names                                                            \n",
       "clean_txt             Dr..ur genuine and cleaver journelist , first ...\n",
       "clean_txt_emoji       genuine cleaver journelist first time one spea...\n",
       "tokenized             [genuine, cleaver, journelist, first, time, on...\n",
       "pos_tags              [(genuine, JJ), (cleaver, NN), (journelist, NN...\n",
       "wordnet_pos           [(genuine, a), (cleaver, n), (journelist, n), ...\n",
       "lemmatized            [genuine, cleaver, journelist, first, time, on...\n",
       "Name: 40953, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#28775 - bad cleaning\n",
    "#40953 - \"\"\n",
    "#24091 - \"\"\n",
    "# 46427 - abbrevaiton check ALR\n",
    "clean_df.iloc[40953]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>profanity_list</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>emoji_face</th>\n",
       "      <th>emoji_names</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>clean_txt_emoji</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td>katandandre mkr</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>In words #katandandre, food crapilicious! #mkr</td>\n",
       "      <td>words food crapilicious</td>\n",
       "      <td>[words, food, crapilicious]</td>\n",
       "      <td>[(words, NNS), (food, NN), (crapilicious, JJ)]</td>\n",
       "      <td>[(words, n), (food, n), (crapilicious, a)]</td>\n",
       "      <td>[word, food, crapilicious]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td>aussietv MKR theblock ImACelebrityAU today sun...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Why #aussietv white? #MKR #theblock #ImACelebr...</td>\n",
       "      <td>white</td>\n",
       "      <td>[white]</td>\n",
       "      <td>[(white, JJ)]</td>\n",
       "      <td>[(white, a)]</td>\n",
       "      <td>[white]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>XochitlSuckkks</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>@XochitlSuckkks classy whore? Or red velvet cu...</td>\n",
       "      <td>classy whore red velvet cupcakes</td>\n",
       "      <td>[classy, whore, red, velvet, cupcakes]</td>\n",
       "      <td>[(classy, NN), (whore, NN), (red, JJ), (velvet...</td>\n",
       "      <td>[(classy, n), (whore, n), (red, a), (velvet, n...</td>\n",
       "      <td>[classy, whore, red, velvet, cupcake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Jason_Gio</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>@Jason_Gio meh. :P thanks heads up, concerned ...</td>\n",
       "      <td>meh thanks heads concerned another angry dude ...</td>\n",
       "      <td>[meh, thanks, heads, concerned, another, angry...</td>\n",
       "      <td>[(meh, JJ), (thanks, NNS), (heads, NNS), (conc...</td>\n",
       "      <td>[(meh, a), (thanks, n), (heads, n), (concerned...</td>\n",
       "      <td>[meh, thanks, head, concern, another, angry, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>RudhoeEnglish</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>@RudhoeEnglish This ISIS account pretending Ku...</td>\n",
       "      <td>isis account pretending kurdish account like i...</td>\n",
       "      <td>[isis, account, pretending, kurdish, account, ...</td>\n",
       "      <td>[(isis, NN), (account, NN), (pretending, VBG),...</td>\n",
       "      <td>[(isis, n), (account, n), (pretending, v), (ku...</td>\n",
       "      <td>[isi, account, pretend, kurdish, account, like...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type  \\\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying   \n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
       "\n",
       "  profanity_list                                           hashtags  \\\n",
       "0                                                   katandandre mkr   \n",
       "1                 aussietv MKR theblock ImACelebrityAU today sun...   \n",
       "2                                                                     \n",
       "3                                                                     \n",
       "4                                                                     \n",
       "\n",
       "         mentions emoji_face emoji_names  \\\n",
       "0                                          \n",
       "1                                          \n",
       "2  XochitlSuckkks                          \n",
       "3       Jason_Gio                          \n",
       "4   RudhoeEnglish                          \n",
       "\n",
       "                                           clean_txt  \\\n",
       "0     In words #katandandre, food crapilicious! #mkr   \n",
       "1  Why #aussietv white? #MKR #theblock #ImACelebr...   \n",
       "2  @XochitlSuckkks classy whore? Or red velvet cu...   \n",
       "3  @Jason_Gio meh. :P thanks heads up, concerned ...   \n",
       "4  @RudhoeEnglish This ISIS account pretending Ku...   \n",
       "\n",
       "                                     clean_txt_emoji  \\\n",
       "0                            words food crapilicious   \n",
       "1                                              white   \n",
       "2                   classy whore red velvet cupcakes   \n",
       "3  meh thanks heads concerned another angry dude ...   \n",
       "4  isis account pretending kurdish account like i...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0                        [words, food, crapilicious]   \n",
       "1                                            [white]   \n",
       "2             [classy, whore, red, velvet, cupcakes]   \n",
       "3  [meh, thanks, heads, concerned, another, angry...   \n",
       "4  [isis, account, pretending, kurdish, account, ...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0     [(words, NNS), (food, NN), (crapilicious, JJ)]   \n",
       "1                                      [(white, JJ)]   \n",
       "2  [(classy, NN), (whore, NN), (red, JJ), (velvet...   \n",
       "3  [(meh, JJ), (thanks, NNS), (heads, NNS), (conc...   \n",
       "4  [(isis, NN), (account, NN), (pretending, VBG),...   \n",
       "\n",
       "                                         wordnet_pos  \\\n",
       "0         [(words, n), (food, n), (crapilicious, a)]   \n",
       "1                                       [(white, a)]   \n",
       "2  [(classy, n), (whore, n), (red, a), (velvet, n...   \n",
       "3  [(meh, a), (thanks, n), (heads, n), (concerned...   \n",
       "4  [(isis, n), (account, n), (pretending, v), (ku...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0                         [word, food, crapilicious]  \n",
       "1                                            [white]  \n",
       "2              [classy, whore, red, velvet, cupcake]  \n",
       "3  [meh, thanks, head, concern, another, angry, d...  \n",
       "4  [isi, account, pretend, kurdish, account, like...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a Steemer or a Lemma\n",
    "##### to stem or to lemm? \n",
    "- Using both and comparing the result\n",
    "- knowing that lemma will go to the root word :; eg, lemma for better is good\n",
    "- Steemma for better is ? - beta?\n",
    "\n",
    "\n",
    "According to Stackoverflow: https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming\n",
    "\n",
    "Lemmatization and stemming are special cases of normalization. They identify a canonical representative for a set of related word forms.\n",
    "\n",
    " Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma\n",
    "\n",
    " The real difference between stemming and lemmatization is threefold:\n",
    "\n",
    "Stemming reduces word-forms to (pseudo)stems, whereas lemmatization reduces the word-forms to linguistically valid lemmas. This difference is apparent in languages with more complex morphology, but may be irrelevant for many IR applications;\n",
    "Lemmatization deals only with inflectional variance, whereas stemming may also deal with derivational variance;\n",
    "In terms of implementation, lemmatization is usually more sophisticated (especially for morphologically complex languages) and usually requires some sort of lexica. Satisfatory stemming, on the other hand, can be achieved with rather simple rule-based approaches.\n",
    "Lemmatization may also be backed up by a part-of-speech tagger in order to disambiguate homonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dsci572env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49c76ddf62ca98de74eedc2bb55b33e27315dcb91d5e2af2ea84b0e3308bb054"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
